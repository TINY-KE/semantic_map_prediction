
import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
import math
import torch
from PIL import Image
from habitat_sim.utils.common import d3_40_colors_rgb
import datasets.util.map_utils as map_utils
import datasets.util.viz_utils as viz_utils
import cv2

'''
MP3D original semantic labels and reduced set correspondence
# Original set from here: https://github.com/niessner/Matterport/blob/master/metadata/mpcat40.tsv
0 void 0
1 wall 15 structure
2 floor 17 free-space
3 chair 1
4 door 2
5 table 3
6 picture 18
7 cabinet 19
8 cushion 4
9 window 15 structure
10 sofa 5
11 bed 6
12 curtain 16 other
13 chest_of_drawers 20
14 plant 7
15 sink 8
16 stairs 17 free-space
17 ceiling 17 free-space
18 toilet 9
19 stool 21
20 towel 22
21 mirror 16 other
22 tv_monitor 10
23 shower 11
24 column 15 structure
25 bathtub 12
26 counter 13
27 fireplace 23
28 lighting 16 other
29 beam 16 other
30 railing 16 other
31 shelving 16 other
32 blinds 16 other
33 gym_equipment 24
34 seating 25
35 board_panel 16 other
36 furniture 16 other
37 appliances 14
38 clothes 26
39 objects 16 other
40 misc 16 other
'''
# 27 categories which include the 21 object categories in the habitat challenge
label_conversion_40_27 = {-1:0, 0:0, 1:15, 2:17, 3:1, 4:2, 5:3, 6:18, 7:19, 8:4, 9:15, 10:5, 11:6, 12:16, 13:20, 14:7, 15:8, 16:17, 17:17,
                    18:9, 19:21, 20:22, 21:16, 22:10, 23:11, 24:15, 25:12, 26:13, 27:23, 28:16, 29:16, 30:16, 31:16, 32:16,
                    33:24, 34:25, 35:16, 36:16, 37:14, 38:26, 39:16, 40:16}


# BGR  # wacv 20230823 the note for the color is wrong!
# color_mapping_27 = {
#     0:(255,255,255), # white
#     1:(128,128,0), # olive (dark yellow)
#     2:(0,0,255), # blue
#     3:(255,0,0), # red
#     4:(255,0,255), # magenta
#     5:(0,255,255), # cyan
#     6:(255,165,0), # orange
#     7:(255,255,0), # yellow
#     8:(128,128,128), # gray
#     9:(128,0,0), # maroon
#     10:(255,20,147), # pink
#     11:(0,128,0), # dark green
#     12:(128,0,128), # purple
#     13:(0,128,128), # teal
#     14:(0,0,128), # navy (dark blue)
#     15:(210,105,30), # chocolate
#     16:(188,143,143), # rosy brown
#     # 17:(0,255,0), # green
#     # 17: (217, 239, 226),  # light green1
#     17: (179, 224, 197),  # light green2
#     # 17: (229, 195, 156),  # light blue
#     18:(255,215,0), # gold
#     19:(0,0,0), # black
#     20:(192,192,192), # silver
#     21:(138,43,226), # blue violet
#     22:(255,127,80), # coral
#     23:(238,130,238), # violet
#     24:(245,245,220), # beige
#     25:(139,69,19), # saddle brown
#     26:(64,224,208) # turquoise
# }


name_mapping_27 = {
    0:  "ç©ºç±»åˆ«",           # ç™½è‰² white                       ç©ºç±»åˆ« / æ— ç±»åˆ« (void)
    1:  "æ¤…å­",             # æ©„æ¦„è‰² olive                     æ¤…å­ (chair)  ***
    2:  "é—¨",               # è“è‰² blue                        é—¨ (door)  ***%%
    3:  "æ¡Œå­",             # çº¢è‰² red                         æ¡Œå­ (table)  ***
    4:  "é å«cushion",             # æ´‹çº¢è‰² magenta                   é å« / åå« (cushion)  ***
    5:  "æ²™å‘",             # é’è‰² cyan                        æ²™å‘ (sofa)  ***
    6:  "åºŠ",               # æ©™è‰² orange                      åºŠ (bed)  ***
    7:  "æ¤ç‰©",             # é»„è‰² yellow                      æ¤ç‰© (plant)
    8:  "æ´—æ‰‹æ± ",           # ç°è‰² gray                        æ´—æ‰‹æ±  / æ°´æ§½ (sink)
    9:  "é©¬æ¡¶",             # æ —è‰² maroon                      é©¬æ¡¶ (toilet)
    10: "ç”µè§†",             # æ·±ç²‰çº¢ deep pink                 ç”µè§† / æ˜¾ç¤ºå™¨ (tv_monitor)  ***%%
    11: "æ·‹æµ´å™¨",           # æ·±ç»¿è‰² dark green               æ·‹æµ´å™¨ (shower)
    12: "æµ´ç¼¸",             # ç´«è‰² purple                      æµ´ç¼¸ (bathtub)  ***%%
    13: "å·¥ä½œå°counter",           # æ°´é¸­è‰² teal                      æ“ä½œå° / å·¥ä½œå° (counter)  ***
    14: "å®¶ç”µ",             # è—é’è‰² navy                     å®¶ç”µ (appliances)
    15: "å¢™",         # å·§å…‹åŠ›è‰² chocolate              å»ºç­‘ç»“æ„ (structure)
    16: "å…¶ä»–",             # è¤ç«ç‘°è‰² rosy brown             å…¶ä»– / æ‚é¡¹ (other)
    17: "å¯è¡Œèµ°åŒºåŸŸ",       # ç»¿è‰² green                      ç©ºé—²ç©ºé—´ / å¯è¡Œèµ°åŒºåŸŸ (free-space)
    18: "ç”»",             # é‡‘è‰² gold                       å›¾ç‰‡ / æŒ‚ç”» (picture)
    19: "æ©±æŸœcabinet",             # é»‘è‰² black                      æ©±æŸœ / æŸœå­ (cabinet)  ***
    20: "æŠ½å±‰",           # é“¶è‰² silver                     æŠ½å±‰æŸœ (chest_of_drawers)
    21: "å‡³å­",             # è“ç´«è‰² blue violet              å‡³å­ (stool)
    22: "æ¯›å·¾",             # çŠç‘šè‰² coral                    æ¯›å·¾ (towel)
    23: "å£ç‚‰",             # ç´«ç½—å…°è‰² violet                 å£ç‚‰ (fireplace)
    24: "å¥èº«å™¨æ",         # ç±³è‰² / æµ…å¡å…¶ beige            å¥èº«å™¨æ (gym_equipment)
    25: "åº§ä½",             # é©¬éæ£• saddle brown            åº§ä½ï¼ˆç»¼åˆç±»ï¼‰(seating)
    26: "è¡£æœ",             # ç»¿æ¾çŸ³è‰² turquoise              è¡£ç‰© (clothes)
}

color_mapping_27 = {
    0:  (255, 255, 255),   # ç™½è‰² white                       ç©ºç±»åˆ« / æ— ç±»åˆ« (void)
    1:  (128, 128, 0),     # æ©„æ¦„è‰² olive                     æ¤…å­ (chair)  ***
    2:  (0, 0, 255),       # è“è‰² blue                        é—¨ (door)  ***
    3:  (255, 0, 0),       # çº¢è‰² red                         æ¡Œå­ (table)  ***
    4:  (255, 0, 255),     # æ´‹çº¢è‰² magenta                   é å« / åå« (cushion)  ***
    5:  (0, 255, 255),     # é’è‰² cyan                        æ²™å‘ (sofa)  ***
    6:  (255, 165, 0),     # æ©™è‰² orange                      åºŠ (bed)  ***
    7:  (255, 255, 0),     # é»„è‰² yellow                      æ¤ç‰© (plant)
    8:  (128, 128, 128),   # ç°è‰² gray                        æ´—æ‰‹æ±  / æ°´æ§½ (sink)
    9:  (128, 0, 0),       # æ —è‰² maroon                      é©¬æ¡¶ (toilet)
    10: (255, 20, 147),    # æ·±ç²‰çº¢ deep pink                 ç”µè§† / æ˜¾ç¤ºå™¨ (tv_monitor)  ***
    11: (0, 128, 0),       # æ·±ç»¿è‰² dark green               æ·‹æµ´å™¨ (shower)
    12: (128, 0, 128),     # ç´«è‰² purple                      æµ´ç¼¸ (bathtub)  ***
    13: (0, 128, 128),     # æ°´é¸­è‰² teal                      æ“ä½œå° / å·¥ä½œå° (counter)  ***
    14: (0, 0, 128),       # è—é’è‰² navy                     å®¶ç”µ (appliances)
    15: (210, 105, 30),    # å·§å…‹åŠ›è‰² chocolate              å»ºç­‘ç»“æ„ (structure)
    16: (188, 143, 143),   # è¤ç«ç‘°è‰² rosy brown             å…¶ä»– / æ‚é¡¹ (other)
    17: (0, 255, 0),       # ç»¿è‰² green                      ç©ºé—²ç©ºé—´ / å¯è¡Œèµ°åŒºåŸŸ (free-space)   $$$
    18: (255, 215, 0),     # é‡‘è‰² gold                       å›¾ç‰‡ / æŒ‚ç”» (picture)
    19: (0, 0, 0),         # é»‘è‰² black                      æ©±æŸœ / æŸœå­ (cabinet)  ***
    20: (192, 192, 192),   # é“¶è‰² silver                     æŠ½å±‰æŸœ (chest_of_drawers)
    21: (138, 43, 226),    # è“ç´«è‰² blue violet              å‡³å­ (stool)
    22: (255, 127, 80),    # çŠç‘šè‰² coral                    æ¯›å·¾ (towel)
    23: (238, 130, 238),   # ç´«ç½—å…°è‰² violet                 å£ç‚‰ (fireplace)
    24: (245, 245, 220),   # ç±³è‰² / æµ…å¡å…¶ beige            å¥èº«å™¨æ (gym_equipment)
    25: (139, 69, 19),     # é©¬éæ£• saddle brown            åº§ä½ï¼ˆç»¼åˆç±»ï¼‰(seating)
    26: (64, 224, 208)     # ç»¿æ¾çŸ³è‰² turquoise              è¡£ç‰© (clothes)
}


# three label classification (0:void, 1:occupied, 2:free)
label_conversion_40_3 = {-1:0, 0:0, 1:1, 2:2, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, 13:1, 14:1, 15:1, 16:2, 17:2,
                    18:1, 19:1, 20:1, 21:1, 22:1, 23:1, 24:1, 25:1, 26:1, 27:1, 28:1, 29:1, 30:1, 31:1, 32:1,
                    33:1, 34:1, 35:1, 36:1, 37:1, 38:1, 39:1, 40:1}
# RGB
color_mapping_3 = {
    0:(200,200,200), # unknown
    1:(0,0,0), # obstacle
    2:(255,255,255), # free
}

# ä¿å­˜å›¾åƒæ•°æ®ä¸º .png æ–‡ä»¶ã€‚
def write_img(img, savepath, name):
    # img: T x 3 x dim x dim, assumed normalized
    for i in range(img.shape[0]):
        vis_img = img[i,:,:,:].cpu().numpy()
        vis_img = np.transpose(vis_img, (1,2,0))
        im_path = savepath + str(i) + "_" + name + ".png"
        cv2.imwrite(im_path, vis_img[:,:,::-1]*255.0)

# ä¿å­˜æ·±åº¦å›¾ä¸º .png æ–‡ä»¶ï¼Œå¹¶åº”ç”¨é¢œè‰²æ˜ å°„ï¼ˆJETï¼‰ã€‚
def write_depth_img(img, savepath, name):
    # img: T x 1 x dim x dim, assumed normalized
    for i in range(img.shape[0]):

        vis_img_tmp = img[i][0].cpu().numpy()
        min = np.min(vis_img_tmp)
        max = np.max(vis_img_tmp)

        # print('vis_img_tmp', vis_img_tmp)
        vis_img = (img[i][0].cpu().numpy() / (max-min)*255).astype(np.uint8)
        # print('vis_img', vis_img)
        # print('vis_img shape', vis_img.shape)
        vis_img_color = cv2.applyColorMap(vis_img, cv2.COLORMAP_JET)

        # vis_img = np.transpose(vis_img, (1,2,0))
        im_path = savepath + str(i) + "_" + name + ".png"
        # cv2.imwrite(im_path, vis_img[:,:,::-1]*255.0)
        # vis_img.save(im_path)
        # cv2.cvtColor()
        cv2.imwrite(im_path, vis_img_color)
        # cv2.imwrite(im_path, vis_img[:,:,::-1]*255.0)


# æŠŠä¸€ä¸ªè¯­ä¹‰ç½‘æ ¼ï¼ˆoccupancy / semantic gridï¼‰ä»å¤šé€šé“æ ‡ç­¾æˆ–æ¦‚ç‡å›¾ï¼Œè½¬æ¢æˆå½©è‰²çš„ RGB å›¾åƒï¼Œæ–¹ä¾¿å¯è§†åŒ–æˆ–å†™å…¥ TensorBoard è§†é¢‘ã€‚
# è¾“å…¥ï¼š grid æ˜¯ä¸€ä¸ªäº”ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º(B, T, C, H, W)
# è¾“å‡ºï¼š (B, T, 3, H, W) çš„å¼ é‡ï¼Œå…¶ä¸­ 3 æ˜¯ RGB é€šé“æ•°ã€‚
def colorize_grid(grid, color_mapping=27): # to pass into tensorboardX video
    # Input: grid -- B x T x C x grid_dim x grid_dim, where C=1,T=1 when gt and C=41,T>=1 for other
    # Output: grid_img -- B x T x 3 x grid_dim x grid_dim
    grid = grid.detach().cpu().numpy()
    grid_img = np.zeros((grid.shape[0], grid.shape[1], grid.shape[3], grid.shape[4], 3),  dtype=np.uint8)

    # å¦‚æœ åŸgrid æœ‰å¤šä¸ªé€šé“ï¼ˆC > 1ï¼‰ï¼Œå–æœ€å¤§æ¦‚ç‡
    if grid.shape[2] > 1:
        # For cells where prob distribution is all zeroes (or uniform), argmax returns arbitrary number (can be true for the accumulated maps)
        # æ¯ä¸ªåƒç´ åœ¨ C ä¸ªé€šé“ä¸Šä»£è¡¨æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼› å–æœ€å¤§æ¦‚ç‡ np.amax(grid, axis=2)ï¼›
        grid_prob_max = np.amax(grid, axis=2)
        # å¦‚æœæœ€å¤§å€¼ â‰¤ 0.05ï¼Œè¯´æ˜è¿™ä¸ªåƒç´ æ‰€æœ‰ç±»åˆ«éƒ½â€œä¸ç¡®å®šâ€ï¼Œå±äºâ€œæœªè§‚æµ‹åŒºåŸŸâ€ï¼›
        inds = np.asarray(grid_prob_max<=0.05).nonzero() # if no label has prob higher than k then assume unobserved
        # æŠŠè¿™äº›åƒç´ çš„ç±»åˆ« 0 é€šé“è®¾ä¸º 1ï¼ˆå¼ºåˆ¶è®¤ä¸ºå®ƒæ˜¯ â€œvoid / unknownâ€ï¼‰ï¼›
        grid[inds[0], inds[1], 0, inds[2], inds[3]] = 1 # assign label 0 (void) to be the dominant label
        # å–æ¯ä¸ªåƒç´ æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ç¼–å·ï¼Œå¾—åˆ°æ•´æ•°æ ‡ç­¾å›¾ã€‚
        grid = np.argmax(grid, axis=2) # B x T x grid_dim x grid_dimã€‚å³ å–äº† axis=2 ä¹‹åï¼Œç¬¬2ç»´ï¼ˆC2ï¼‰å°±è¢«â€œå‹æ‰â€äº†ã€‚
    else:
        grid = grid.squeeze(2)

    if color_mapping==27:
        color_mapping = color_mapping_27
    else:
        color_mapping = color_mapping_3
    for label in color_mapping.keys():
        grid_img[ grid==label ] = color_mapping[label]

    #å½“å‰ grid_img æ˜¯ (B, T, H, W, 3)ï¼Œé€šè¿‡ transpose(0, 1, 4, 2, 3) â†’ (B, T, 3, H, W)
    return torch.tensor(grid_img.transpose(0, 1, 4, 2, 3), dtype=torch.uint8)


def write_tensor_imgSegm(img, savepath, name, t=None):
    # pred: T x C x dim x dim
    if img.shape[1] > 1:
        img = torch.argmax(img.cpu(), dim=1, keepdim=True) # T x 1 x cH x cW
    img_labels = img.squeeze(1)

    for i in range(img_labels.shape[0]):
        img0 = img_labels[i,:,:]

        vis_img = np.zeros((img0.shape[0], img0.shape[1], 3), dtype=np.uint8)
        for label in color_mapping_27.keys():
            vis_img[ img0==label ] = color_mapping_27[label]
        
        if t is None:
            im_path = savepath + str(i) + "_" + name + ".png"
        else:
            im_path = savepath + name + "_" + str(t) + "_" + str(i) + ".png"
        # cv2.imwrite(im_path, vis_img[:,:,::-1])
        cv2.imwrite(im_path, vis_img)



def display_sample(rgb_obs, depth_obs, sseg_img=None, savepath=None):
    # sseg_img is semantic observation from Matterport habitat
    depth_obs = depth_obs / np.amax(depth_obs) # normalize for visualization
    rgb_img = Image.fromarray(rgb_obs, mode="RGB")
    depth_img = Image.fromarray((depth_obs * 255).astype(np.uint8), mode="L")
    
    if sseg_img is not None:
        semantic_img = Image.new("P", (sseg_img.shape[1], sseg_img.shape[0]))
        semantic_img.putpalette(d3_40_colors_rgb.flatten())
        semantic_img.putdata((sseg_img.flatten() % 40).astype(np.uint8))
        semantic_img = semantic_img.convert("RGBA")

        arr = [rgb_img, depth_img, semantic_img]
        n=3
    else:
        arr = [rgb_img, depth_img]
        n=2

    plt.figure(figsize=(12, 8))
    for i, data in enumerate(arr):
        ax = plt.subplot(1, n, i+1)
        ax.axis('off')
        plt.imshow(data)
    if savepath is None:
        plt.show()
    else:
        plt.savefig(savepath, bbox_inches='tight', pad_inches=0, dpi=100)
    plt.close()


# zhjd å®šåˆ¶
#  ensemble_object_maps.shape:  torch.Size([3, 1, 10, 27, 64, 64])  ensemble_num, B, T, C, cH, cW
#  pred_maps_objects.shape:  torch.Size([1, 10, 27, 64, 64]  B, T, _, cH, cW
def save_ensembles(ensemble_object_maps, pred_maps_objects, save_img_dir_):
    B, T, _, cH, cW = pred_maps_objects.shape
    for t in range(T):
        ensemble1 = color_and_extract(ensemble_object_maps[0, 0, t, :, :, :], 27)
        ensemble2 = color_and_extract(ensemble_object_maps[1, 0, t, :, :, :], 27)
        ensemble3 = color_and_extract(ensemble_object_maps[2, 0, t, :, :, :], 27)
        ensemble4 = color_and_extract(ensemble_object_maps[3, 0, t, :, :, :], 27)
        ## FIXME: å…ˆç”¨å¹³å‡å›¾æ›¿ä»£
        # pred_maps_objects_single = color_and_extract(pred_maps_objects[0, t, :, :, :], 27)
        # ensemble4 = pred_maps_objects_single

        # === å››å®«æ ¼ä¿å­˜æœ¬åœ°===
        fig, axs = plt.subplots(2, 2, figsize=(20, 20))
        axs = axs.flatten()

        imgs = [
            (ensemble1, "ensemble1", None),
            (ensemble2, "ensemble2", None),
            (ensemble3, "ensemble3", None),
            (ensemble4, "ensemble4", None)
        ]

        for i, (img, title, cmap) in enumerate(imgs):
            axs[i].imshow(img, cmap=cmap)
            axs[i].set_title(title)
            axs[i].axis('off')

        plt.tight_layout()

        # === ä¿å­˜å›¾ç‰‡ ===
        save_file = os.path.join(save_img_dir_, f"ensemble_t{t}.png")
        plt.savefig(save_file, bbox_inches='tight', pad_inches=0, dpi=200)
        plt.close()
    print(f"âœ… å·²ä¿å­˜é›†æˆæ¨¡å‹çš„ç»“æœ: {save_img_dir_}")


# zhjd å®šåˆ¶
#  step_geo_grid.shape:  torch.Size([1, 10, 27, 300, 300])
#  step_uncertainty.shape:  torch.Size([1, 10, 27, 300, 300]
def save_uncertainty(step_geo_grid, step_uncertainty, pose_coords_list, save_img_dir_, timestamp_length):
# def save_uncertainty(sg, ltg, pose_coords, save_img_dir_, timestamp_length):
    step_geo_grid = step_geo_grid.squeeze(0)  # å˜ä¸º[10, 27, 300, 300]
    step_uncertainty = step_uncertainty.squeeze(0)  # å˜ä¸º[10, 27, 300, 300]
    for sem_lbl in [1, 3, 4, 5, 6, 13, 19]:
        class_name = name_mapping_27.get(sem_lbl, "æœªçŸ¥ç±»åˆ«")
        for t in range(timestamp_length):
            # 1. æå–è¯¥ç±»åˆ«çš„é¢„æµ‹å›¾ï¼ˆæ¦‚ç‡å›¾ï¼‰
            target_pred = step_geo_grid[t, sem_lbl, :, :].unsqueeze(0)  # [1, H, W]
            # ZHJD: å°†ç­‰äº 1/C çš„ä½ç½®ç½®ä¸º 0. å»é™¤å±å¹•è¾¹ç¼˜çš„é»„è‰²åŒºåŸŸï¼Œä¸ºäº†ç¾è§‚
            mask = (target_pred == (1.0 / 27.0))
            target_pred[mask] = 0.0
            target_pred = target_pred.permute(1, 2, 0).cpu().numpy() * 255.0

            # 2. æå–è¯¥ç±»åˆ«çš„ä¸ç¡®å®šæ€§å›¾
            target_uncertainty = step_uncertainty[t, sem_lbl, :, :].unsqueeze(0)
            target_uncertainty = target_uncertainty.permute(1, 2, 0).cpu().numpy()
            target_uncertainty /= np.amax(target_uncertainty)+ 1e-6  # é¿å…é™¤ 0
            target_uncertainty = target_uncertainty * 255.0
            #  3. è·å–æ•´ä¸ªè¯­ä¹‰åœ°å›¾çš„å½©è‰²å›¾
            # color_sem_grid = colorize_grid(sg.sem_grid.unsqueeze(1))
            # im = color_sem_grid[0, 0, :, :, :].permute(1, 2, 0).cpu().numpy()
            color_sem_grid = colorize_grid(step_geo_grid[t].unsqueeze(0).unsqueeze(0))  # shape: [1, 1, H, W, 3]
            im = color_sem_grid[0, 0].permute(1, 2, 0).cpu().numpy()

            #  5. è£å‰ªä¸­å¿ƒåŒºåŸŸï¼ˆ100x100ï¼‰
            # crop viz inputs to 128 x 128
            area_size = 100  # area around the agent to be evaluated
            # åªå…³æ³¨ agent å‘¨å›´çš„å±€éƒ¨åŒºåŸŸï¼Œé¿å…å›¾å¤ªå¤§
            area_start = int((im.shape[0] / 2) - (area_size / 2))
            area_end = int((im.shape[0] / 2) + (area_size / 2))
            # æŠŠå½©è‰²è¯­ä¹‰å›¾ã€ä¸ç¡®å®šæ€§å›¾ã€é¢„æµ‹å›¾éƒ½è£æˆ 100x100
            im = im[area_start:area_end, area_start:area_end, :]
            target_uncertainty = target_uncertainty[area_start:area_end, area_start:area_end, :]
            target_pred = target_pred[area_start:area_end, area_start:area_end, :]

            #  6. å¹³ç§»åæ ‡ï¼ˆåŒ¹é…è£å‰ªååæ ‡ç³»ï¼‰
            # translate coords   å‡å» area_start æ˜¯ä¸ºäº†æŠŠåæ ‡å¯¹é½åˆ°è£å‰ªåçš„å›¾åƒä¸­
            # ltg[0, 0, 0] -= area_start
            # ltg[0, 0, 1] -= area_start
            pose_x = pose_coords_list[t, 0, 0, 0].item() - area_start
            pose_y = pose_coords_list[t, 0, 0, 1].item() - area_start

            # 7. å¯è§†åŒ–å¹¶ä¿å­˜å›¾ç‰‡
            # æŠŠä¸‰ä¸ªå›¾ï¼ˆè¯­ä¹‰åœ°å›¾ã€é¢„æµ‹å›¾ã€ä¸ç¡®å®šæ€§å›¾ï¼‰ç”¨ matplotlib æ‹¼æˆ 3 ä¸ª subplot
            # å…¶ä¸­ç¬¬ä¸€å¼ å›¾ä¸Šæ·»åŠ äº† agent å½“å‰çš„ä½ç½®ï¼ˆè“è‰²ï¼‰å’Œç›®æ ‡ç‚¹ä½ç½®ï¼ˆæ´‹çº¢è‰²ï¼‰
            arr = [im, target_pred, target_uncertainty]
            plt.figure(figsize=(20, 15))
            for i, data in enumerate(arr):
                ax = plt.subplot(1, 3, i + 1)
                ax.axis('off')
                plt.imshow(data)
                if i == 0:
                    plt.scatter(pose_x, pose_y, color="blue", s=50)
                    # plt.scatter(ltg[0, 0, 0], ltg[0, 0, 1], color="magenta", s=50)

            # 8. ä¿å­˜å›¾åƒä¸º PNG
            filename = f"{class_name}_time-{t}.png"
            filepath = save_img_dir_ + filename
            plt.savefig(filepath, bbox_inches='tight', pad_inches=0, dpi=200)
            plt.close()
    print(f"âœ… å·²ä¿å­˜æ–¹å·®çš„ç»“æœ: {save_img_dir_}")


# å°†è¯­ä¹‰åœ°å›¾ï¼ˆsemantic mapï¼‰ã€é¢„æµ‹ç»“æœå’Œä¸ç¡®å®šæ€§å›¾å¯è§†åŒ–å¹¶ä¿å­˜ä¸ºä¸€å¼ å›¾ç‰‡
# test_ds	æµ‹è¯•é›†å¯¹è±¡ï¼Œå«ç‚¹äº‘ç­‰ä¿¡æ¯
# sg	è¯­ä¹‰åœ°å›¾å¯¹è±¡ï¼ˆsemantic gridï¼‰ï¼Œå«æœ‰ sem_grid å’Œ per_class_uncertainty_map
# sem_lbl	è¦å¯è§†åŒ–çš„è¯­ä¹‰ç±»åˆ«ç´¢å¼•ï¼ˆå¦‚â€œæ¡Œå­â€ã€â€œæ¤…å­â€ç­‰ï¼‰
# abs_pose	å½“å‰ agent çš„ä¸–ç•Œåæ ‡ä½ç½®
# ltg	long-term goalï¼ˆç›®æ ‡ç‚¹ï¼‰åæ ‡
# pose_coords	å½“å‰ agent åœ¨æ …æ ¼åœ°å›¾ä¸­çš„åæ ‡
# agent_height	agent èº«é«˜ï¼Œç”¨äºæŠ•å½±ç‚¹äº‘
# save_img_dir_	ä¿å­˜å›¾ç‰‡çš„è·¯å¾„å‰ç¼€
# t	å½“å‰æ—¶é—´æ­¥ç¼–å·ï¼ˆç”¨äºå‘½åï¼‰
def save_visual_steps(test_ds, sg, sem_lbl, abs_pose, ltg, pose_coords, agent_height, save_img_dir_, t):
    # 1. æå–è¯¥ç±»åˆ«çš„é¢„æµ‹å›¾ï¼ˆæ¦‚ç‡å›¾ï¼‰
    target_pred = sg.sem_grid[:,sem_lbl,:,:]
    target_pred = target_pred.permute(1,2,0).cpu().numpy()*255.0
    # 2. æå–è¯¥ç±»åˆ«çš„ä¸ç¡®å®šæ€§å›¾
    target_uncertainty = sg.per_class_uncertainty_map[:,sem_lbl,:,:].permute(1,2,0).cpu().numpy()
    target_uncertainty /= np.amax(target_uncertainty)
    target_uncertainty = target_uncertainty*255.0
    #  3. è·å–æ•´ä¸ªè¯­ä¹‰åœ°å›¾çš„å½©è‰²å›¾
    color_sem_grid = colorize_grid(sg.sem_grid.unsqueeze(1))
    im = color_sem_grid[0,0,:,:,:].permute(1,2,0).cpu().numpy()
    #  4. è·å–åœ°é¢çœŸå®è¯­ä¹‰ crop å›¾ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    pose_ = np.asarray(abs_pose).reshape(1,3)
    gt_grid_crops_objects = map_utils.get_gt_crops(pose_, test_ds.pcloud, test_ds.label_seq_objects, agent_height,
                                            test_ds.grid_dim, test_ds.crop_size, test_ds.cell_size)
    color_gt_crop = colorize_grid(gt_grid_crops_objects.unsqueeze(0))
    im_gt_crop = color_gt_crop[0,0,:,:,:].permute(1,2,0).cpu().numpy()

    #  5. è£å‰ªä¸­å¿ƒåŒºåŸŸï¼ˆ100x100ï¼‰
    # crop viz inputs to 128 x 128
    area_size = 100 # area around the agent to be evaluated
    area_start = int( (im.shape[0] / 2) - (area_size / 2) )
    area_end = int( (im.shape[0] / 2) + (area_size / 2) )
    im = im[area_start:area_end, area_start:area_end,:]
    target_uncertainty = target_uncertainty[area_start:area_end, area_start:area_end,:]
    target_pred = target_pred[area_start:area_end, area_start:area_end,:]

    # translate coords
    ltg[0,0,0] -= area_start
    ltg[0,0,1] -= area_start
    pose_coords[0,0,0] -= area_start
    pose_coords[0,0,1] -= area_start

    # 7. å¯è§†åŒ–å¹¶ä¿å­˜å›¾ç‰‡
    # æŠŠä¸‰ä¸ªå›¾ï¼ˆè¯­ä¹‰åœ°å›¾ã€é¢„æµ‹å›¾ã€ä¸ç¡®å®šæ€§å›¾ï¼‰ç”¨ matplotlib æ‹¼æˆ 3 ä¸ª subplot
    # å…¶ä¸­ç¬¬ä¸€å¼ å›¾ä¸Šæ·»åŠ äº† agent å½“å‰çš„ä½ç½®ï¼ˆè“è‰²ï¼‰å’Œç›®æ ‡ç‚¹ä½ç½®ï¼ˆæ´‹çº¢è‰²ï¼‰
    arr = [ im,
            target_pred,
            target_uncertainty
            ]
    n=len(arr)
    plt.figure(figsize=(20 ,15))
    for i, data in enumerate(arr):
        ax = plt.subplot(1, 3, i+1)
        ax.axis('off')
        plt.imshow(data)
        if i==0:
            plt.scatter(ltg[0,0,0], ltg[0,0,1], color="magenta", s=50)
            plt.scatter(pose_coords[0,0,0], pose_coords[0,0,1], color="blue", s=50)

    # 8. ä¿å­˜å›¾åƒä¸º PNG
    plt.savefig(save_img_dir_+str(t)+'.png', bbox_inches='tight', pad_inches=0, dpi=200)
    plt.close()


# def save_map_pred_steps(spatial_in, spatial_pred, objects_pred, ego_img_segm, save_img_dir_, t):
#
#     color_spatial_in = colorize_grid(spatial_in.unsqueeze(0), color_mapping=3)
#     im_spatial_in = color_spatial_in[0,0,:,:,:].permute(1,2,0).cpu().numpy()
#
#     color_spatial_pred = colorize_grid(spatial_pred, color_mapping=3)
#     im_spatial_pred = color_spatial_pred[0,0,:,:,:].permute(1,2,0).cpu().numpy()
#
#     color_objects_pred = colorize_grid(objects_pred, color_mapping=27)
#     im_objects_pred = color_objects_pred[0,0,:,:,:].permute(1,2,0).cpu().numpy()
#
#     color_ego_img_segm = colorize_grid(ego_img_segm, color_mapping=27)
#     im_ego_img_segm = color_ego_img_segm[0,0,:,:,:].permute(1,2,0).cpu().numpy()
#
#     arr = [ im_spatial_in,
#             im_spatial_pred,
#             im_objects_pred,
#             im_ego_img_segm
#             ]
#     n=len(arr)
#     plt.figure(figsize=(20 ,15))
#     for i, data in enumerate(arr):
#         ax = plt.subplot(1, n, i+1)
#         ax.axis('off')
#         plt.imshow(data)
#     plt.savefig(save_img_dir_+"map_step_"+str(t)+'.png', bbox_inches='tight', pad_inches=0, dpi=200)
#     plt.close()







# zhjd
def add_border(img, color=(255, 0, 0), thickness=5):
    # å¦‚æœæ˜¯ (C,H,W)ï¼Œè½¬ä¸º (H,W,C)
    if img.ndim == 3 and img.shape[0] == 3:
        img = img.permute(1, 2, 0)

    if isinstance(img, torch.Tensor):
        img = img.detach().cpu().numpy()

    img = (img * 255).astype(np.uint8) if img.max() <= 1 else img.copy()

    h, w, c = img.shape
    img[:thickness, :, :] = color  # top
    img[-thickness:, :, :] = color  # bottom
    img[:, :thickness, :] = color  # left
    img[:, -thickness:, :] = color  # right
    return img

def to_5d(t):
    t = torch.tensor(t)
    while t.ndim < 5:
        t = t.unsqueeze(0)  # åœ¨æœ€å‰é¢æ·»åŠ ä¸€ä¸ªæ–°ç»´åº¦ã€‚ä¾‹å¦‚åŸæ¥æ˜¯ (64, 64) â†’ å˜æˆ (1, 64, 64)
    return t

fix_extract = 0

# === ç”¨ colorize_grid ä¸Šè‰² ===
def color_and_extract(grid, color_mapping):
    colorized = colorize_grid(to_5d(grid), color_mapping=color_mapping)
    # è¾“å‡ºå¯èƒ½æ˜¯ (3,H,W) æˆ– (1,3,H,W) æˆ– (1,1,3,H,W)
    colorized = torch.tensor(colorized)
    # å°†äº”/å››ç»´åº¦è½¬ä¸ºä¸‰ç»´åº¦
    if colorized.ndim == 5:
        colorized = colorized[0, fix_extract]  # é»˜è®¤æ˜¾ç¤ºçš„æ˜¯ç¬¬äºŒç»´ï¼ˆtimeï¼‰çš„ç¬¬ 0 å¸§ã€‚
    elif colorized.ndim == 4:
        colorized = colorized[fix_extract]
    # ç°åœ¨ colorized åº”ä¸º (3,H,W)
    colorized.permute(1, 2, 0) # è½¬ä¸º (H,W,3)
    colorized_border = add_border(colorized, color=(10, 10, 10), thickness=1)
    return colorized_border

def show_image_color_and_extract(tensor_or_array, title="image", color_mapping=27):
    if isinstance(tensor_or_array, torch.Tensor):
        img = tensor_or_array.detach().cpu().numpy()
    else:
        img = np.array(tensor_or_array)
    img = color_and_extract(img, color_mapping=color_mapping)
    plt.imshow(img)
    plt.title(title)
    plt.axis('off')
    plt.show()

def show_image(tensor_or_array, title="image"):
    if isinstance(tensor_or_array, torch.Tensor):
        img = tensor_or_array.detach().cpu().numpy()
    else:
        img = np.array(tensor_or_array)
    if img.ndim == 3 and img.shape[0] in [1, 3]:
        img = np.transpose(img, (1, 2, 0))
    plt.imshow(img)
    plt.title(title)
    plt.axis('off')
    plt.show()


def show_image_sseg_2d_label(tensor_or_array, title="image"):

    if isinstance(tensor_or_array, torch.Tensor):
        img = tensor_or_array.detach().cpu().numpy()
    else:
        img = np.array(tensor_or_array)

    # 2ï¸âƒ£ ç¡®ä¿æ˜¯äºŒç»´æ ‡ç­¾å›¾ [H,W]
    # assert img.ndim == 2, f" [zhjd-debug] Expected 2D label map, got shape {img.shape}"
    if img.ndim == 5:
        img = img[0, fix_extract, 0]  # é»˜è®¤æ˜¾ç¤ºçš„æ˜¯ç¬¬äºŒç»´ï¼ˆtimeï¼‰çš„ç¬¬ 0 å¸§ã€‚
    elif img.ndim == 4:
        img = img[0, fix_extract]
    elif img.ndim == 4:
        img = img[fix_extract]

    H, W = img.shape
    rgb_img = np.zeros((H, W, 3), dtype=np.uint8)

    # 3ï¸âƒ£ å°†æ¯ä¸ª label è½¬æˆ RGB
    for lbl, color in color_mapping_27.items():
        mask = img == lbl
        rgb_img[mask] = color

    # 4ï¸âƒ£ æ˜¾ç¤ºç»“æœ
    plt.imshow(rgb_img)
    plt.title(title)
    plt.axis('off')
    plt.show()



def save_all_infos_and_mapprediction_origin(batch, pred_maps_objects, savepath, name):
    # batch: ['abs_pose', 'ego_grid_crops_spatial', 'step_ego_grid_crops_spatial', 'gt_grid_crops_spatial', 'gt_grid_crops_objects', 'images', 'ssegs', 'depth_imgs', 'pred_ego_crops_sseg', 'step_ego_grid_27']
    images = batch['images']
    ssegs = batch['gt_segm']  # ç‰©ä½“åˆ†å‰²çœŸå€¼
    depth_imgs = batch['depth_imgs']
    pred_ego_crops_sseg = batch['pred_ego_crops_sseg']  # net3çš„è¾“å‡º

    step_ego_grid_27 = batch['step_ego_grid_27']
    ##### RSMPçš„è¾“å‡º pred_maps_objects
    gt_grid_crops_objects = batch['gt_grid_crops_objects']

    ego_grid_crops_spatial = batch['ego_grid_crops_spatial']  # å½“å‰å¸§å‡ ä½•åœ°å›¾
    step_ego_grid_crops_spatial = batch['step_ego_grid_crops_spatial']  # å¤šå¸§èåˆå‡ ä½•åœ°å›¾
    gt_grid_crops_spatial = batch['gt_grid_crops_spatial']  # è¯­ä¹‰åœ°å›¾çœŸå€¼

    B, T, _, cH, cW = step_ego_grid_27.shape
    for t in range(T):
        # print(f"ğŸ•’ æ—¶é—´æ­¥ {t}")
        images_single = images[0, t, :, :, :].detach().cpu().permute(1, 2, 0).numpy()
        ssegs_single = ssegs[0, t, :, :, :].detach().cpu().permute(1, 2, 0).numpy()
        depth_imgs_single = depth_imgs[0, t, :, :, :].detach().cpu().permute(1, 2, 0).numpy()

        step_ego_grid_27_single = color_and_extract(step_ego_grid_27[0, t, :, :, :], 27)
        pred_maps_objects_single = color_and_extract(pred_maps_objects[0, t, :, :, :], 27)
        gt_grid_crops_objects_single = color_and_extract(gt_grid_crops_objects[0, t, :, :, :], 27)

        ego_grid_crops_spatial_single = color_and_extract(ego_grid_crops_spatial[0, t, :, :, :], 3)
        step_ego_grid_crops_spatial_single = color_and_extract(step_ego_grid_crops_spatial[0, t, :, :, :], 3)
        gt_grid_crops_spatial_single = color_and_extract(gt_grid_crops_spatial[0, t, :, :, :], 3)

        # fig, axs = plt.subplots(3, 3, figsize=(20, 20))
        # axs = axs.flatten()
        #
        # axs[0].imshow(images_single)
        # axs[0].set_title("RGB Image")
        #
        # axs[1].imshow(ssegs_single, cmap='tab20')
        # axs[1].set_title("GT Segmentation")
        #
        # axs[2].imshow(depth_imgs_single, cmap='viridis')
        # axs[2].set_title("Depth")
        #
        # axs[3].imshow(step_ego_grid_27_single, cmap='viridis')
        # axs[3].set_title("RGB Project")
        #
        # axs[4].imshow(pred_maps_objects_single, cmap='viridis')
        # axs[4].set_title("Refined Semantic Map")
        #
        # axs[5].imshow(gt_grid_crops_objects_single, cmap='viridis')
        # axs[5].set_title("GT Semantic Map")
        #
        # axs[6].imshow(ego_grid_crops_spatial_single, cmap='viridis')
        # axs[6].set_title("Depth Project")
        #
        # axs[7].imshow(step_ego_grid_crops_spatial_single, cmap='viridis')
        # axs[7].set_title("Refined Occupied Map")
        #
        # axs[8].imshow(gt_grid_crops_spatial_single, cmap='viridis')
        # axs[8].set_title("GT Occupied Map")
        #
        # for ax in axs:
        #     ax.axis('off')
        #
        # plt.tight_layout()
        # plt.show()

        # === ä¹å®«æ ¼ä¿å­˜æœ¬åœ°===
        fig, axs = plt.subplots(3, 3, figsize=(20, 20))
        axs = axs.flatten()

        imgs = [
            (images_single, "RGB Image", None),
            (ssegs_single, "GT Segmentation", 'tab20'),
            (depth_imgs_single, "Depth", 'viridis'),
            (step_ego_grid_27_single, "RGB Project", None),
            (pred_maps_objects_single, "Refined Semantic Map", None),
            (gt_grid_crops_objects_single, "GT Semantic Map", None),
            (ego_grid_crops_spatial_single, "Depth Project", None),
            (step_ego_grid_crops_spatial_single, "Refined Occupied Map", None),
            (gt_grid_crops_spatial_single, "GT Occupied Map", None)
        ]

        for i, (img, title, cmap) in enumerate(imgs):
            axs[i].imshow(img, cmap=cmap)
            axs[i].set_title(title)
            axs[i].axis('off')

        plt.tight_layout()

        # === ä¿å­˜å›¾ç‰‡ ===
        save_file = os.path.join(savepath, f"{name}_t{t}.png")
        plt.savefig(save_file, bbox_inches='tight', pad_inches=0, dpi=200)
        plt.close()
        print(f"âœ… å·²ä¿å­˜: {save_file}")



def save_all_infos_and_mapprediction_slam(batch, pred_maps_objects, savepath, name):
    # batch: ['abs_pose', 'ego_grid_crops_spatial', 'step_ego_grid_crops_spatial', 'gt_grid_crops_spatial', 'gt_grid_crops_objects', 'images', 'ssegs', 'depth_imgs', 'pred_ego_crops_sseg', 'step_ego_grid_27']
    images = batch['images']
    ssegs = batch['gt_segm']  # ç‰©ä½“åˆ†å‰²çœŸå€¼
    depth_imgs = batch['depth_imgs']
    step_ego_grid_27 = batch['step_ego_grid_27']



    B, T, _, cH, cW = step_ego_grid_27.shape
    for t in range(T):
        # print(f"ğŸ•’ æ—¶é—´æ­¥ {t}")
        # RGBå›¾
        images_single = images[0, t, :, :, :].detach().cpu().numpy()

        # --- è¯­ä¹‰å›¾ä¸Šè‰² ---
        ssegs_single = ssegs[0, t, :, :].detach().cpu().numpy()
        segm_color = colorize_sseg(ssegs_single, color_mapping_27)

        # æ·±åº¦å›¾ä¸Šè‰²
        depth_imgs_single = depth_imgs[0, t, :, :].detach().cpu().numpy()
        depth_norm = cv2.normalize(depth_imgs_single, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        depth_color = cv2.applyColorMap(depth_norm, cv2.COLORMAP_JET)  # (H, W, 3)

        # step_ego_grid_27_single = color_and_extract(step_ego_grid_27[0, t, :, :, :], 27)
        # pred_maps_objects_single = color_and_extract(pred_maps_objects[0, t, :, :, :], 27)  # RSMPçš„è¾“å‡º pred_maps_objects
        step_ego_grid_27_single = colorEncode(step_ego_grid_27[0, t, :, :, :].argmax(axis=0))
        pred_maps_objects_single = colorEncode(pred_maps_objects[0, t, :, :, :].argmax(axis=0))


        # === å…­å®«æ ¼ä¿å­˜æœ¬åœ°===
        fig, axs = plt.subplots(3, 2, figsize=(30, 30))
        axs = axs.flatten()

        imgs = [
            (images_single, "RGB Image", None),
            (ssegs_single, "GT Segmentation", 'tab20'),
            (depth_imgs_single, "Depth", 'viridis'),
            (step_ego_grid_27_single, "RGB Project", None),
            (pred_maps_objects_single, "Refined Semantic Map", None),
        ]

        for i, (img, title, cmap) in enumerate(imgs):
            axs[i].imshow(img, cmap=cmap)
            axs[i].set_title(title)
            axs[i].axis('off')

        plt.tight_layout()

        # === ä¿å­˜å›¾ç‰‡ ===
        save_file = os.path.join(savepath, f"{name}_t{t}.png")
        plt.savefig(save_file, bbox_inches='tight', pad_inches=0, dpi=200)
        plt.close()
        print(f"âœ… å·²ä¿å­˜: {save_file}")



def save_all_infos_and_mapprediction_Global(batch, local_pred_maps_objects, global_maps_objects, savepath, name):
    # batch: ['abs_pose', 'ego_grid_crops_spatial', 'step_ego_grid_crops_spatial', 'gt_grid_crops_spatial',
    # 'gt_grid_crops_objects', 'images', 'ssegs', 'depth_imgs', 'pred_ego_crops_sseg', 'step_ego_grid_27']
    images = batch['images']
    ssegs = batch['gt_segm']  # ç‰©ä½“åˆ†å‰²çœŸå€¼
    depth_imgs = batch['depth_imgs']
    pred_ego_crops_sseg = batch['pred_ego_crops_sseg']  # net3çš„è¾“å‡º

    step_ego_grid_27 = batch['step_ego_grid_27']
    ##### RSMPçš„è¾“å‡º pred_maps_objects
    gt_grid_crops_objects = batch['gt_grid_crops_objects']

    ego_grid_crops_spatial = batch['ego_grid_crops_spatial']  # å½“å‰å¸§å‡ ä½•åœ°å›¾
    step_ego_grid_crops_spatial = batch['step_ego_grid_crops_spatial']  # å¤šå¸§èåˆå‡ ä½•åœ°å›¾
    gt_grid_crops_spatial = batch['gt_grid_crops_spatial']  # è¯­ä¹‰åœ°å›¾çœŸå€¼

    B, T, _, cH, cW = step_ego_grid_27.shape
    for t in range(T):
        # print(f"ğŸ•’ æ—¶é—´æ­¥ {t}")
        images_single = images[0, t, :, :, :].detach().cpu().permute(1, 2, 0).numpy()
        ssegs_single = ssegs[0, t, :, :, :].detach().cpu().permute(1, 2, 0).numpy()
        depth_imgs_single = depth_imgs[0, t, :, :, :].detach().cpu().permute(1, 2, 0).numpy()

        step_ego_grid_27_single = color_and_extract(step_ego_grid_27[0, t, :, :, :], 27)
        pred_maps_objects_single = color_and_extract(local_pred_maps_objects[0, t, :, :, :], 27)
        gt_grid_crops_objects_single = color_and_extract(gt_grid_crops_objects[0, t, :, :, :], 27)

        global_maps_objects_single = color_and_extract(global_maps_objects[0, t, :, :, :], 27)
        # ego_grid_crops_spatial_single = color_and_extract(ego_grid_crops_spatial[0, t, :, :, :], 3)
        step_ego_grid_crops_spatial_single = color_and_extract(step_ego_grid_crops_spatial[0, t, :, :, :], 3)
        gt_grid_crops_spatial_single = color_and_extract(gt_grid_crops_spatial[0, t, :, :, :], 3)


        # === ä¹å®«æ ¼ä¿å­˜æœ¬åœ°===
        fig, axs = plt.subplots(3, 3, figsize=(20, 20))
        axs = axs.flatten()

        imgs = [
            (images_single, "RGB Image", None),
            (ssegs_single, "GT Segmentation", 'tab20'),
            (depth_imgs_single, "Depth", 'viridis'),
            (step_ego_grid_27_single, "RGB Project", None),
            (pred_maps_objects_single, "Refined Semantic Map", None),
            (gt_grid_crops_objects_single, "GT Semantic Map", None),
            (global_maps_objects_single, "Global Semantic Map", None),
            (step_ego_grid_crops_spatial_single, "Refined Occupied Map", None),
            (gt_grid_crops_spatial_single, "GT Occupied Map", None)
        ]

        for i, (img, title, cmap) in enumerate(imgs):
            axs[i].imshow(img, cmap=cmap)
            axs[i].set_title(title)
            axs[i].axis('off')

        plt.tight_layout()

        # === ä¿å­˜å›¾ç‰‡ ===
        save_file = os.path.join(savepath, f"{name}_t{t}.png")
        plt.savefig(save_file, bbox_inches='tight', pad_inches=0, dpi=200)
        plt.close()
    print(f"âœ… å·²ä¿å­˜è¾“å…¥ä¿¡æ¯å’ŒEGOè¯­ä¹‰åœ°å›¾: {savepath}")


# zhjd
def colorEncode(label_map, color_mapping=color_mapping_27):
    """
    å°†å•é€šé“æ ‡ç­¾å›¾è½¬æ¢ä¸ºå½©è‰²å›¾åƒï¼ˆRGBï¼‰ã€‚

    å‚æ•°:
        label_map: numpy.ndarray, shape (H, W)ï¼Œæ¯ä¸ªåƒç´ æ˜¯ç±»åˆ« ID
        color_mapping: dict[int, tuple[int, int, int]]ï¼Œç±»åˆ« ID â†’ RGB é¢œè‰²

    è¿”å›:
        RGB å›¾åƒ: numpy.ndarray, shape (H, W, 3)ï¼Œdtype=uint8
    """
    # ä¿è¯è¾“å…¥æ˜¯ numpyï¼Œå¹¶ squeeze æ‰å¤šä½™ç»´åº¦
    if isinstance(label_map, torch.Tensor):
        label_map = label_map.detach().cpu().numpy()

    label_map = np.squeeze(label_map)  # å»æ‰å¤šä½™ç»´åº¦ï¼Œä¾‹å¦‚ (1, H, W) â†’ (H, W)

    if label_map.ndim != 2:
        raise ValueError(f"[colorEncode] è¾“å…¥çš„ label_map å¿…é¡»æ˜¯ 2Dï¼Œä½†å®é™…æ˜¯ {label_map.shape}")

    h, w = label_map.shape
    color_img = np.zeros((h, w, 3), dtype=np.uint8)

    for label_id, color in color_mapping.items():
        color_img[label_map == label_id] = color

    return color_img




# zhjd
def colorEncode(label_map):
    """
    å°†å•é€šé“æ ‡ç­¾å›¾è½¬æ¢ä¸ºå½©è‰²å›¾åƒï¼ˆRGBï¼‰ã€‚

    å‚æ•°:
        label_map: numpy.ndarray, shape (H, W)ï¼Œæ¯ä¸ªåƒç´ æ˜¯ç±»åˆ« ID
        color_mapping: dict[int, tuple[int, int, int]]ï¼Œç±»åˆ« ID â†’ RGB é¢œè‰²

    è¿”å›:
        RGB å›¾åƒ: numpy.ndarray, shape (H, W, 3)ï¼Œdtype=uint8
    """
    color_mapping = color_mapping_27

    # ä¿è¯è¾“å…¥æ˜¯ numpyï¼Œå¹¶ squeeze æ‰å¤šä½™ç»´åº¦
    if isinstance(label_map, torch.Tensor):
        label_map = label_map.detach().cpu().numpy()

    label_map = np.squeeze(label_map)  # å»æ‰å¤šä½™ç»´åº¦ï¼Œä¾‹å¦‚ (1, H, W) â†’ (H, W)

    if label_map.ndim != 2:
        raise ValueError(f"[colorEncode] è¾“å…¥çš„ label_map å¿…é¡»æ˜¯ 2Dï¼Œä½†å®é™…æ˜¯ {label_map.shape}")

    h, w = label_map.shape
    color_img = np.zeros((h, w, 3), dtype=np.uint8)

    for label_id, color in color_mapping.items():
        color_img[label_map == label_id] = color

    return color_img


def colorize_sseg(sseg, color_map):
    h, w = sseg.shape
    color_image = np.zeros((h, w, 3), dtype=np.uint8)

    for label_id, color in color_map.items():
        mask = sseg == label_id
        color_image[mask] = color

    return color_image