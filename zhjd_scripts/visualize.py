import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from torch.utils.data import Dataset
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import habitat
from habitat.config.default import get_config
import habitat.utils.visualizations.maps as map_util
from datasets.util import map_utils
import datasets.util.utils as utils
import datasets.util.viz_utils as viz_utils
from models.img_segmentation import get_img_segmentor_from_options
import test_utils as tutils

import torchvision.transforms as transforms

import gzip
import json


# ----------------------------
# æ•°æ®é›†ç±»
# ----------------------------
class ObjNavEpisodeDataset(Dataset):
    def __init__(self, episode_files):
        self.episodes_file_list = episode_files

    def __len__(self):
        return len(self.episodes_file_list)

    def length(self):
        return len(self.episodes_file_list)

    def __getitem__(self, idx):
        ep_file = self.episodes_file_list[idx]
        ep = np.load(ep_file)

        abs_pose = ep['abs_pose'][-10:]
        ego_grid_crops_spatial = torch.from_numpy(ep['ego_grid_crops_spatial'][-10:])
        step_ego_grid_crops_spatial = torch.from_numpy(ep['step_ego_grid_crops_spatial'][-10:])
        gt_grid_crops_spatial = torch.from_numpy(ep['gt_grid_crops_spatial'][-10:])
        gt_grid_crops_objects = torch.from_numpy(ep['gt_grid_crops_objects'][-10:])

        # è®¡ç®—ç›¸å¯¹ä½å§¿
        rel_pose = []
        for i in range(abs_pose.shape[0]):
            rel_pose.append(utils.get_rel_pose(pos2=abs_pose[i], pos1=abs_pose[0]))

        item = {
            'pose': torch.from_numpy(np.asarray(rel_pose)).float(),
            'abs_pose': torch.from_numpy(abs_pose).float(),
            'ego_grid_crops_spatial': ego_grid_crops_spatial,
            'step_ego_grid_crops_spatial': step_ego_grid_crops_spatial,
            'gt_grid_crops_spatial': gt_grid_crops_spatial,
            'gt_grid_crops_objects': gt_grid_crops_objects,

            'images': torch.from_numpy(ep['images'][-10:]),
            'gt_segm': torch.from_numpy(ep['ssegs'][-10:]).type(torch.int64),
            'depth_imgs': torch.from_numpy(ep['depth_imgs'][-10:]),

            'pred_ego_crops_sseg': torch.from_numpy(ep['pred_ego_crops_sseg'][-10:]),
            'step_ego_grid_27': torch.from_numpy(ep['step_ego_grid_27'][-10:])
        }

        return item

# ----------------------------
# å¯è§†åŒ–å‡½æ•°
# ----------------------------
def visualize_item(item, timestep=0):
    rgb = item['images'][timestep]          # [3, H, W]
    # rgb_np = normalize_rgb(rgb).transpose(1, 2, 0)  # [H, W, 3]
    segm = item['gt_segm'][timestep][0]     # [H, W]
    depth = item['depth_imgs'][timestep][0] # [H, W]

    rgb_np = rgb.permute(1, 2, 0).numpy().astype(np.uint8)
    segm_np = segm.numpy()
    depth_np = depth.numpy()

    fig, axs = plt.subplots(1, 3, figsize=(15, 5))
    axs[0].imshow(rgb_np)
    axs[0].set_title("RGB Image")
    axs[0].axis("off")

    axs[1].imshow(segm_np, cmap="tab20")
    axs[1].set_title("Semantic Segmentation")
    axs[1].axis("off")

    axs[2].imshow(depth_np, cmap="viridis")
    axs[2].set_title("Depth Map")
    axs[2].axis("off")

    plt.tight_layout()
    plt.show()

def normalize_rgb(rgb_tensor):
    """
    å°† RGB float tensor è½¬æ¢ä¸º uint8 æ˜¾ç¤ºå›¾åƒ
    æ”¯æŒèŒƒå›´ä¸º [0, 1] æˆ– [-1, 1]
    """
    rgb_np = rgb_tensor.detach().cpu().numpy()
    if rgb_np.dtype in [np.float32, np.float64]:
        if rgb_np.max() <= 1.0 and rgb_np.min() >= 0.0:
            rgb_np = rgb_np * 255.0
        elif rgb_np.min() >= -1.0 and rgb_np.max() <= 1.0:
            rgb_np = (rgb_np + 1.0) * 127.5
    rgb_np = np.clip(rgb_np, 0, 255).astype(np.uint8)
    return rgb_np

def tensor_to_np(t):
    """ç¡®ä¿ Tensor æ˜¯ numpy æ ¼å¼ï¼Œä¸” squeeze æ‰ batch/channel ç»´åº¦"""
    if isinstance(t, torch.Tensor):
        t = t.detach().cpu()
        if t.ndim == 4:
            t = t[0]
        if t.ndim == 3 and t.shape[0] == 1:
            t = t[0]
        return t.numpy()
    return t


def visualize_all_fields_colorized(item, timestep=0):
    """
    ä½¿ç”¨ viz_utils.colorize_grid å¯è§†åŒ– episode çš„å…³é”®å­—æ®µã€‚
    è‡ªåŠ¨è¡¥é½è¾“å…¥ç»´åº¦å¹¶ç»Ÿä¸€è¾“å‡ºç»´åº¦ä»¥é€‚é… imshowã€‚
    """

    # === å–æ•°æ® ===
    rgb = normalize_rgb(item['images'][timestep]).transpose(1, 2, 0)
    segm = tensor_to_np(item['gt_segm'][timestep])
    depth = tensor_to_np(item['depth_imgs'][timestep])
    pred_semantic_grid_map = tensor_to_np(item['pred_ego_crops_sseg'][timestep])
    ego_spatial = tensor_to_np(item['ego_grid_crops_spatial'][timestep])
    step_ego_spatial = tensor_to_np(item['step_ego_grid_crops_spatial'][timestep])
    gt_spatial = tensor_to_np(item['gt_grid_crops_spatial'][timestep])
    gt_objects = tensor_to_np(item['gt_grid_crops_objects'][timestep])
    step_grid_27 = tensor_to_np(item['step_ego_grid_27'][timestep])

    # # === å¦‚æœé¢„æµ‹è¯­ä¹‰æ˜¯ CÃ—HÃ—Wï¼Œå– argmax ===
    # if pred_semantic_grid_map.ndim == 3 and pred_semantic_grid_map.shape[0] > 1:
    #     pred_semantic_grid_map = np.argmax(pred_semantic_grid_map, axis=0)

    def to_5d(t):
        t = torch.tensor(t)
        while t.ndim < 5:
            t = t.unsqueeze(0)  # åœ¨æœ€å‰é¢æ·»åŠ ä¸€ä¸ªæ–°ç»´åº¦ã€‚ä¾‹å¦‚åŸæ¥æ˜¯ (64, 64) â†’ å˜æˆ (1, 64, 64)
        return t

    # === ç”¨ colorize_grid ä¸Šè‰² ===
    def color_and_extract(grid, color_mapping):
        colorized = viz_utils.colorize_grid(to_5d(grid), color_mapping=color_mapping)
        # è¾“å‡ºå¯èƒ½æ˜¯ (3,H,W) æˆ– (1,3,H,W) æˆ– (1,1,3,H,W)
        colorized = torch.tensor(colorized)
        if colorized.ndim == 5:
            colorized = colorized[0, 0]
        elif colorized.ndim == 4:
            colorized = colorized[0]
        # ç°åœ¨ colorized åº”ä¸º (3,H,W)
        return colorized.permute(1, 2, 0)  # è½¬ä¸º (H,W,3)

    color_ego_spatial = color_and_extract(ego_spatial, 3)
    color_step_spatial = color_and_extract(step_ego_spatial, 3)
    color_gt_spatial = color_and_extract(gt_spatial, 3)
    color_gt_objects = color_and_extract(gt_objects, 27)
    color_pred_semantic = color_and_extract(pred_semantic_grid_map, 27)
    color_step_grid27 = color_and_extract(step_grid_27.argmax(axis=0), 27)

    # === ç»˜å›¾ ===
    fig, axs = plt.subplots(3, 3, figsize=(20, 20))
    axs = axs.flatten()

    axs[0].imshow(rgb)
    axs[0].set_title("RGB Image")

    axs[1].imshow(segm, cmap='tab20')
    axs[1].set_title("GT Segmentation")

    axs[2].imshow(depth, cmap='viridis')
    axs[2].set_title("Depth")





    axs[3].imshow(color_pred_semantic)
    axs[3].set_title("Predicted Semantic ")


    axs[4].imshow(color_step_grid27)
    axs[4].set_title("L2M results Input (step_ego_grid_27)")

    axs[5].imshow(color_gt_objects)
    axs[5].set_title("GT Objects ")



    axs[6].imshow(color_ego_spatial)
    axs[6].set_title("Egocentric Spatial ")

    axs[7].imshow(color_step_spatial)
    axs[7].set_title("Step Ego Spatial (step_ego_grid_crops_spatial)")

    axs[8].imshow(color_gt_spatial)
    axs[8].set_title("GT Spatial ")



    for ax in axs:
        ax.axis('off')

    plt.tight_layout()
    plt.show()

    print(f"âœ… timestep={timestep} å¯è§†åŒ–å®Œæˆã€‚")



# ----------------------------
# ä¸»å‡½æ•°å…¥å£
# ----------------------------
if __name__ == "__main__":
    root_path = "/home/robotlab/dataset/semantic/semantic_datasets/data_v6/test/2azQ1b91cZZ"
    ep_path = root_path + '/' + 'ep_1_1_2azQ1b91cZZ.npz'

    # ep_path = '/home/robotlab/dataset/MP3D_dataset/v1/tasks/mp3d_habitat/NPZ/train/HxpKQynjfin/ep_1_1_HxpKQynjfin.npz'
    if not os.path.exists(ep_path):
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {ep_path}")
        exit(1)

    data = np.load(ep_path)
    print(data.files)
    # work1: ['abs_pose', 'ego_grid_crops_spatial', 'step_ego_grid_crops_spatial', 'gt_grid_crops_spatial', 'gt_grid_crops_objects', 'images', 'ssegs', 'depth_imgs']
    # work2: ['abs_pose', 'ego_grid_crops_spatial', 'step_ego_grid_crops_spatial', 'gt_grid_crops_spatial', 'gt_grid_crops_objects', 'images', 'ssegs', 'depth_imgs', 'pred_ego_crops_sseg', 'step_ego_grid_27']
    # models/predictors/map_predictor_model.py ä¸­çš„å”¯ä¸€è¾“å…¥æ˜¯step_ego_grid_27, è¿™ä¹Ÿæ˜¯work1çš„è¾“å‡ºã€‚
    # 2ï¸âƒ£ æ‰“å° abs_pose çš„ shape
    print("abs_pose shape:", data['abs_pose'].shape)  # åºåˆ—é•¿åº¦ä¸º10ï¼Œå‚è€ƒL2Mçš„store_episodes_parallel.py, è®¾å®šäº†æ¯ä¸ªepisodeï¼ˆè½¨è¿¹ï¼‰ä¸­åªä¿ç•™æœ€åçš„10ä¸ªposeã€‚

    # 3ï¸âƒ£ è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆæ—¶é—´æ­¥æ•°é‡ï¼‰
    print("abs_pose åºåˆ—é•¿åº¦:", data['abs_pose'].shape[0])

    print(f"{'step_ego_grid_crops_spatial shape:':<35} {data['step_ego_grid_crops_spatial'].shape}")
    print(f"{'step_ego_grid_27 shape:':<35} {data['step_ego_grid_27'].shape}")


    dataset = ObjNavEpisodeDataset([ep_path])
    item = dataset[0]

    step_ego_crops = item['ego_grid_crops_spatial']
    T, _, cH, cW = step_ego_crops.shape
    print(f"T (timesteps): {T}")
    print(f"C (channels): {_}")
    print(f"cH (crop height): {cH}")
    print(f"cW (crop width): {cW}")

    # for t in range(4):
    #     print(f"\n=== å¯è§†åŒ–æ—¶é—´æ­¥ {t} ===")
    #     visualize_item(item, timestep=t)

    for t in range(10):
        print(f"ğŸ•’ æ—¶é—´æ­¥ {t}")
        visualize_all_fields_colorized(item, timestep=t)